<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Life of a Memory Request: HBM to Register</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <style>
        body { font-family: 'Inter', sans-serif; background-color: #0f172a; color: #e2e8f0; }
        .nv-green { color: #76B900; }
        .card {
            background-color: #1e293b;
            border: 1px solid #334155;
            border-radius: 0.75rem;
            padding: 1.5rem;
            margin-bottom: 1.5rem;
        }
        .mermaid { background-color: transparent !important; }
        .mermaid svg { max-width: 100%; }
        .toc-link { border-left: 2px solid transparent; transition: all 0.2s ease; }
        .toc-link:hover { border-left-color: #76B900; background: rgba(118, 185, 0, 0.1); }
    </style>
</head>
<body class="min-h-screen antialiased">
    <div class="max-w-6xl mx-auto px-6 py-12">
        <header class="mb-12">
            <h1 class="text-4xl font-bold mb-4">
                Life of a Memory Request: <span class="nv-green">HBM to Register</span>
            </h1>
            <p class="text-xl text-slate-400">
                How a load instruction travels through the hierarchy—sizes, latencies, and throughput at each level (H100-oriented).
            </p>
            <div class="mt-4 text-sm text-slate-500">
                Builds on: <a href="cuda_execution_memory_hierarchy.md" class="text-blue-400 hover:underline">CUDA Execution &amp; Memory Hierarchy</a> |
                <a href="memory-hierarchy.html" class="text-blue-400 hover:underline">GPU Memory Hierarchy</a> |
                <a href="TMA.html" class="text-blue-400 hover:underline">Tensor Memory Accelerator (TMA)</a>
            </div>
        </header>

        <div class="grid lg:grid-cols-4 gap-8">
            <aside class="lg:col-span-1">
                <nav class="sticky top-6 bg-slate-800/50 rounded-xl p-4 border border-slate-700">
                    <h2 class="text-sm font-semibold text-slate-400 uppercase tracking-wider mb-4">Contents</h2>
                    <ul class="space-y-1 text-sm">
                        <li><a href="#overview" class="toc-link block py-1.5 px-3 rounded text-slate-300">Overview</a></li>
                        <li><a href="#diagram" class="toc-link block py-1.5 px-3 rounded text-slate-300">Diagram: Sizes, Latencies, Throughput</a></li>
                        <li><a href="#dsmem" class="toc-link block py-1.5 px-3 rounded text-slate-300">Distributed Shared Memory (Hopper / Blackwell)</a></li>
                        <li><a href="#tma-path" class="toc-link block py-1.5 px-3 rounded text-slate-300">Alternative Path: TMA</a></li>
                        <li><a href="#path" class="toc-link block py-1.5 px-3 rounded text-slate-300">Request Path Step-by-Step</a></li>
                        <li><a href="#table" class="toc-link block py-1.5 px-3 rounded text-slate-300">Reference Table</a></li>
                    </ul>
                </nav>
            </aside>

            <main class="lg:col-span-3 space-y-10">
                <section id="overview" class="card scroll-mt-6">
                    <h2 class="text-2xl font-semibold mb-4 nv-green">Overview</h2>
                    <p class="text-slate-400 mb-4">
                        When a thread issues a <strong class="text-white">load</strong> from global memory (e.g. <code class="bg-slate-700 px-1.5 py-0.5 rounded">LDG</code>), the request does not go straight to HBM. It passes through the cache hierarchy. Each level has a <strong class="text-slate-300">size</strong> (capacity), <strong class="text-slate-300">latency</strong> (cycles to satisfy a hit), and <strong class="text-slate-300">throughput</strong> (bandwidth). On a miss, the request goes to the next level until it reaches HBM; data then flows back up into the requesting thread’s register.
                    </p>
                    <p class="text-slate-400">
                        The diagram below shows one path: <strong>Register (request) → L1 → L2 → HBM</strong>, with typical <strong>sizes</strong>, <strong>latencies</strong>, and <strong>throughput</strong> for each level (H100 SXM–style numbers). On <strong class="nv-green">Hopper (H100) and Blackwell (B200)</strong>, an additional level—<strong>Distributed Shared Memory (DSMEM)</strong>—lets SMs in a thread block cluster access each other’s shared memory without going through L2. Actual cycles vary by GPU and access pattern; use these as a mental model.
                    </p>
                </section>

                <section id="diagram" class="card scroll-mt-6">
                    <h2 class="text-2xl font-semibold mb-4 nv-green">Diagram: Sizes, Latencies, Throughput</h2>
                    <p class="text-slate-400 mb-4">
                        Request flows <strong>down</strong> on a miss; data flows <strong>back up</strong> into the register. Solid arrows: normal load path. <strong class="nv-green">Dashed: alternative path (TMA)</strong> — Hopper+ bulk copy from HBM/L2 into shared memory, bypassing registers. Numbers are approximate (H100-class GPU).
                    </p>
                    <div class="mermaid">
flowchart LR
    subgraph NORMAL["Normal path (via L1 → Register)"]
        direction TB
        R["<b>Register</b><br/>────────────<br/>Size: private, ~65K regs/SM total<br/>Latency: ~1 cycle (operand read)<br/>Throughput: ~20 TB/s effective"]
        L1N["<b>L1 / Shared</b><br/>────────────<br/>Size: 128–256 KB per SM<br/>Latency: ~20–30 cycles (hit)<br/>Throughput: ~12–19 TB/s"]
        L2N["<b>L2</b><br/>────────────<br/>Size: 50–96 MB (whole GPU)<br/>Latency: ~200 cycles (hit)<br/>Throughput: ~6–8 TB/s<br/>Line: 128 B"]
        H["<b>HBM (Global)</b><br/>────────────<br/>Size: 80–192 GB<br/>Latency: ~400–800 cycles (DRAM)<br/>Throughput: ~3.35 TB/s (H100)"]
    end

    subgraph ALT["Alternative: TMA (no register path)"]
        direction TB
        TMA["<b>TMA Unit</b> (Hopper+)<br/>────────────<br/>Bulk copy, register bypass"]
        SH["<b>Shared Memory</b><br/>(TMA destination)"]
    end

    R -->|"1. Request (miss)"| L1N
    L1N -->|"2. L1 miss"| L2N
    L2N -->|"3. L2 miss"| H
    H -->|"4. Data return"| L2N
    L2N -->|"5. Fill L1"| L1N
    L1N -->|"6. To register"| R

    L2N -.->|"alt: TMA copy"| TMA
    TMA -.->|"alt: to shared"| SH

    style R fill:#76B900,stroke:#76B900,color:#000
    style L1N fill:#22c55e,stroke:#22c55e,color:#000
    style L2N fill:#3b82f6,stroke:#3b82f6,color:#fff
    style H fill:#6366f1,stroke:#6366f1,color:#fff
    style TMA fill:#76B900,stroke:#76B900,color:#000
    style SH fill:#22c55e,stroke:#22c55e,color:#000
                    </div>
                    <p class="text-slate-500 text-sm mt-3">
                        <strong class="text-slate-400">Alternative (TMA):</strong> Data can flow HBM → L2 → TMA unit → Shared Memory without passing through the register file. One thread initiates the bulk copy; see <a href="TMA.html" class="text-blue-400 hover:underline">Tensor Memory Accelerator (TMA)</a> for details.
                    </p>
                </section>

                <section id="dsmem" class="card scroll-mt-6">
                    <h2 class="text-2xl font-semibold mb-4 nv-green">Distributed Shared Memory (Hopper / Blackwell)</h2>
                    <p class="text-slate-400 mb-4">
                        On <strong class="text-white">H100 (Hopper)</strong> and <strong class="text-white">B200 (Blackwell)</strong>, there is an additional level of hierarchy: <strong class="nv-green">Distributed Shared Memory (DSMEM)</strong>. In a <strong class="text-slate-300">Thread Block Cluster</strong>, threads can <strong class="text-slate-300">read and write the shared memory of other SMs</strong> in the same cluster via a high-speed <strong class="text-slate-300">SM-to-SM interconnect</strong>. That path does <strong>not</strong> go through L2, so latency for cluster-wide shared data is lower than going to L2 or HBM—which matters for complex, multi-block kernels like <strong class="text-slate-300">FlashAttention</strong> and other attention or producer-consumer patterns. For another alternative path—bulk copy from HBM to shared memory without going through registers—see <a href="TMA.html" class="text-blue-400 hover:underline">Alternative path: TMA</a> below.
                    </p>
                    <div class="mermaid">
flowchart LR
    subgraph CLUSTER["Thread Block Cluster (Hopper/Blackwell)"]
        SM1["SM 1<br/>Local Shared"]
        SM2["SM 2<br/>Local Shared"]
        SM3["SM 3<br/>Local Shared"]
        SM1 <-->|"DSMEM<br/>SM-to-SM"| SM2
        SM2 <-->|"DSMEM"| SM3
        SM3 <-->|"DSMEM"| SM1
    end
    style SM1 fill:#22c55e,stroke:#22c55e,color:#000
    style SM2 fill:#22c55e,stroke:#22c55e,color:#000
    style SM3 fill:#22c55e,stroke:#22c55e,color:#000
                    </div>
                    <p class="text-slate-500 text-sm mt-3">
                        SMs in the same cluster can exchange data over DSMEM without hitting L2, reducing latency compared to going through global memory or L2.
                    </p>
                    <div class="mt-6 p-4 bg-slate-800/60 rounded-lg border-l-4 border-amber-500/60">
                        <p class="text-sm text-slate-300 italic">
                            “In addition to this hierarchy, on Hopper and Blackwell, I also consider Distributed Shared Memory, which allows SMs to exchange data without hitting the L2, further reducing latency for complex kernels like FlashAttention.”
                        </p>
                        <p class="text-xs text-slate-500 mt-2">— Interview flex: shows you know the full hierarchy including DSMEM and its impact on modern attention kernels.</p>
                    </div>
                </section>

                <section id="tma-path" class="card scroll-mt-6">
                    <h2 class="text-2xl font-semibold mb-4 nv-green">Alternative Path: TMA</h2>
                    <p class="text-slate-400 mb-4">
                        On <strong class="text-white">Hopper (H100)</strong> and later, the <strong class="nv-green">Tensor Memory Accelerator (TMA)</strong> provides a different data path: <strong>HBM → L2 → Shared Memory</strong> via a dedicated TMA unit, <strong class="text-slate-300">bypassing the register file and the normal load pipeline</strong>. This path is shown as the dashed “alt: TMA” edges in the diagram above. One thread initiates a bulk copy; data lands directly in shared memory without passing through registers. The path still goes through L2; only registers (and the L1 load path) are bypassed. For full details, NCU metrics, and kernel names, see <a href="TMA.html" class="text-blue-400 hover:underline font-medium">Tensor Memory Accelerator (TMA)</a>.
                    </p>
                </section>

                <section id="path" class="card scroll-mt-6">
                    <h2 class="text-2xl font-semibold mb-4 nv-green">Request Path Step-by-Step</h2>
                    <ol class="list-decimal list-inside text-slate-400 space-y-3">
                        <li><strong class="text-slate-300">Thread issues load.</strong> The warp executes a load instruction; the address is in a register. The request is sent to L1 (or the memory pipeline that checks L1).</li>
                        <li><strong class="text-slate-300">L1 lookup.</strong> If the line is in L1, data is returned in ~20–30 cycles and written to the destination register. If <strong class="text-amber-400">L1 miss</strong>, the request goes to L2.</li>
                        <li><strong class="text-slate-300">L2 lookup.</strong> If the line is in L2, it is fetched in ~200 cycles, fills L1, and then data reaches the register. If <strong class="text-amber-400">L2 miss</strong>, the request goes to HBM.</li>
                        <li><strong class="text-slate-300">HBM (DRAM) access.</strong> The controller fetches a cache line (e.g. 128 bytes) from HBM. Latency is hundreds of cycles (e.g. 400–800); throughput is limited by HBM bandwidth (~3.35 TB/s on H100).</li>
                        <li><strong class="text-slate-300">Data returns.</strong> Data flows back: HBM → L2 (line filled) → L1 (line filled) → register. The thread’s warp may have been descheduled while waiting; when data is ready, the warp can be scheduled again and the register gets the value.</li>
                    </ol>
                    <p class="text-slate-500 text-sm mt-4">
                        Because latency is high for L2 and HBM, GPUs hide it by running many warps: when one warp stalls on a load, the scheduler runs another warp. See <a href="cuda_execution_memory_hierarchy.md" class="text-blue-400 hover:underline">CUDA Execution &amp; Memory Hierarchy</a> (Warp Scheduler).
                    </p>
                </section>

                <section id="table" class="card scroll-mt-6">
                    <h2 class="text-2xl font-semibold mb-4 nv-green">Reference Table</h2>
                    <p class="text-slate-400 mb-4">Approximate values for a modern datacenter GPU (H100-class). Latencies are in cycles; actual values depend on GPU and workload.</p>
                    <div class="overflow-x-auto">
                        <table class="w-full text-sm">
                            <thead>
                                <tr class="text-left text-slate-400 border-b border-slate-600">
                                    <th class="py-3 pr-4">Level</th>
                                    <th class="py-3 pr-4">Size</th>
                                    <th class="py-3 pr-4">Latency (hit)</th>
                                    <th class="py-3 pr-4">Throughput</th>
                                </tr>
                            </thead>
                            <tbody class="text-slate-300">
                                <tr class="border-b border-slate-600">
                                    <td class="py-3 pr-4 font-medium">Register</td>
                                    <td class="py-3 pr-4">~65K 32-bit regs per SM (shared by threads)</td>
                                    <td class="py-3 pr-4">~1 cycle</td>
                                    <td class="py-3 pr-4">~20 TB/s effective</td>
                                </tr>
                                <tr class="border-b border-slate-600">
                                    <td class="py-3 pr-4 font-medium">L1 / Shared</td>
                                    <td class="py-3 pr-4">128–256 KB per SM</td>
                                    <td class="py-3 pr-4">~20–30 cycles</td>
                                    <td class="py-3 pr-4">~12–19 TB/s</td>
                                </tr>
                                <tr class="border-b border-slate-600">
                                    <td class="py-3 pr-4 font-medium">DSMEM (cluster shared)</td>
                                    <td class="py-3 pr-4">Shared memory of other SMs in same cluster (H100/B200)</td>
                                    <td class="py-3 pr-4">Lower than L2; SM-to-SM interconnect</td>
                                    <td class="py-3 pr-4">High; bypasses L2</td>
                                </tr>
                                <tr class="border-b border-slate-600">
                                    <td class="py-3 pr-4 font-medium">L2</td>
                                    <td class="py-3 pr-4">50–96 MB (whole GPU)</td>
                                    <td class="py-3 pr-4">~200 cycles</td>
                                    <td class="py-3 pr-4">~6–8 TB/s</td>
                                </tr>
                                <tr class="border-b border-slate-600">
                                    <td class="py-3 pr-4 font-medium">HBM (Global)</td>
                                    <td class="py-3 pr-4">80–192 GB</td>
                                    <td class="py-3 pr-4">~400–800+ cycles</td>
                                    <td class="py-3 pr-4">~3.35 TB/s (H100 SXM)</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                </section>
            </main>
        </div>

        <footer class="mt-12 pt-8 border-t border-slate-700 text-center text-sm text-slate-500">
            <p>Part of the <a href="../index.html" class="text-blue-400 hover:underline">AI Performance Design Guide</a></p>
            <p class="mt-2">Related: <a href="cuda_execution_memory_hierarchy.md" class="text-blue-400 hover:underline">CUDA Execution &amp; Memory Hierarchy</a> | <a href="memory-hierarchy.html" class="text-blue-400 hover:underline">Memory Hierarchy</a> | <a href="TMA.html" class="text-blue-400 hover:underline">Tensor Memory Accelerator (TMA)</a></p>
        </footer>
    </div>

    <script>
        mermaid.initialize({
            startOnLoad: true,
            theme: 'dark',
            themeVariables: {
                primaryColor: '#76B900',
                primaryTextColor: '#fff',
                primaryBorderColor: '#76B900',
                lineColor: '#94a3b8',
                secondaryColor: '#1e293b',
                tertiaryColor: '#334155',
                background: '#0f172a',
                mainBkg: '#1e293b',
                nodeBorder: '#334155',
                clusterBkg: '#1e293b',
                clusterBorder: '#334155',
                defaultLinkColor: '#94a3b8',
                nodeTextColor: '#e2e8f0'
            }
        });
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function(e) {
                e.preventDefault();
                const target = document.querySelector(this.getAttribute('href'));
                if (target) target.scrollIntoView({ behavior: 'smooth', block: 'start' });
            });
        });
    </script>
</body>
</html>
