<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Advanced Optimization & Profiling Techniques for LLM Training</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #0f172a;
            color: #e2e8f0;
        }
        .nv-green { color: #76B900; }
        .bg-nv-green { background-color: #76B900; }
        .border-nv-green { border-color: #76B900; }
        
        .code-block {
            background-color: #1e293b;
            border: 1px solid #334155;
            color: #e2e8f0;
            padding: 1rem;
            border-radius: 0.5rem;
            font-family: 'JetBrains Mono', monospace;
            white-space: pre-wrap;
            font-size: 0.875rem;
            line-height: 1.7;
            overflow-x: auto;
        }
        .code-block .comment { color: #64748b; }
        .code-block .keyword { color: #f472b6; }
        .code-block .string { color: #a5f3fc; }
        .code-block .variable { color: #fbbf24; }
        
        .info-card {
            background: linear-gradient(135deg, #1e3a2f 0%, #1e293b 100%);
            border-radius: 0.75rem;
            border-left: 4px solid #76B900;
        }
        .warning-card {
            background: linear-gradient(135deg, #451a03 0%, #1e293b 100%);
            border-radius: 0.75rem;
            border-left: 4px solid #f59e0b;
        }
        
        nav a {
            transition: all 0.2s ease;
        }
        nav a:hover {
            color: #76B900;
        }
        
        .toc-link {
            border-left: 2px solid transparent;
            transition: all 0.2s ease;
        }
        .toc-link:hover {
            border-left-color: #76B900;
            background: rgba(118, 185, 0, 0.1);
        }
        
        .tech-card {
            background: linear-gradient(145deg, rgba(30, 41, 59, 0.8) 0%, rgba(15, 23, 42, 0.9) 100%);
            border: 1px solid rgba(71, 85, 105, 0.5);
            border-radius: 0.75rem;
            transition: all 0.2s ease;
        }
        .tech-card:hover {
            border-color: rgba(118, 185, 0, 0.3);
        }
        
        .ref-link {
            color: #76B900;
            text-decoration: none;
            border-bottom: 1px solid transparent;
            transition: border-color 0.2s ease;
        }
        .ref-link:hover {
            border-bottom-color: #76B900;
        }
    </style>
</head>
<body class="antialiased">
    <div class="container mx-auto p-4 sm:p-6 lg:p-8 max-w-6xl">
        <!-- Header -->
        <header class="text-center mb-12">
            <div class="inline-flex items-center gap-3 mb-4">
                <svg class="w-10 h-10 nv-green" viewBox="0 0 24 24" fill="currentColor">
                    <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm-2 15l-5-5 1.41-1.41L10 14.17l7.59-7.59L19 8l-9 9z"/>
                </svg>
                <h1 class="text-3xl sm:text-4xl font-bold text-white">Advanced Optimization & Profiling Techniques</h1>
            </div>
            <p class="text-xl text-slate-400">LLM Training on NVIDIA Grace Hopper</p>
            <p class="mt-2 text-sm text-slate-500">AMP, CPU offloading, ZeRO, CUDA Graphs, KV cache offload, selective profiling</p>
        </header>

        <div class="grid lg:grid-cols-4 gap-8">
            <!-- Table of Contents Sidebar -->
            <aside class="lg:col-span-1">
                <nav class="sticky top-6 bg-slate-800/50 rounded-xl p-4 border border-slate-700">
                    <h2 class="text-sm font-semibold text-slate-400 uppercase tracking-wider mb-4">Contents</h2>
                    <ul class="space-y-1 text-sm">
                        <li><a href="#amp" class="toc-link block py-1.5 px-3 rounded text-slate-300">AMP / Mixed Precision (FP16/BF16)</a></li>
                        <li><a href="#cpu-offload" class="toc-link block py-1.5 px-3 rounded text-slate-300">CPU Offloading</a></li>
                        <li><a href="#zero" class="toc-link block py-1.5 px-3 rounded text-slate-300">ZeRO</a></li>
                        <li><a href="#cuda-graphs" class="toc-link block py-1.5 px-3 rounded text-slate-300">CUDA Graphs</a></li>
                        <li><a href="#kv-cache" class="toc-link block py-1.5 px-3 rounded text-slate-300">KV Cache Offload</a></li>
                        <li><a href="#selective-profiling" class="toc-link block py-1.5 px-3 rounded text-slate-300">Selective Profiling</a></li>
                        <li><a href="#references" class="toc-link block py-1.5 px-3 rounded text-slate-300">References</a></li>
                    </ul>
                </nav>
            </aside>

            <!-- Main Content -->
            <main class="lg:col-span-3 space-y-12">
                
                <!-- AMP / Mixed Precision -->
                <section id="amp" class="scroll-mt-6">
                    <h2 class="text-2xl font-bold text-white mb-6 flex items-center gap-3">
                        <span class="w-1.5 h-8 bg-nv-green rounded-full"></span>
                        AMP / Mixed Precision (FP16/BF16)
                    </h2>
                    
                    <div class="tech-card p-6 mb-4">
                        <p class="text-slate-300 mb-4">
                            <strong class="text-white">Automatic Mixed Precision (AMP)</strong> keeps most of training in <strong class="nv-green">FP16</strong> or <strong class="nv-green">BF16</strong>, using FP32 only where needed (e.g. loss scaling, numerically sensitive ops). This reduces memory use and increases throughput while keeping training stable.
                        </p>
                        <ul class="text-slate-400 text-sm space-y-2 list-disc list-inside">
                            <li>Nearly halves GPU memory for activations and gradients vs FP32</li>
                            <li>Leverages Tensor Cores (FP16/BF16) for faster matrix math</li>
                            <li>Typically used with a <strong class="text-slate-300">GradScaler</strong> to avoid underflow when scaling loss for backprop</li>
                            <li>BF16 is often preferred for LLM training: same exponent range as FP32, fewer precision issues than FP16</li>
                        </ul>
                    </div>
                    <div class="info-card p-5">
                        <p class="text-slate-300 text-sm"><strong class="text-white">In practice:</strong> Frameworks like PyTorch (<code class="bg-slate-700 px-1.5 py-0.5 rounded">torch.cuda.amp</code>), Lightning (<code class="bg-slate-700 px-1.5 py-0.5 rounded">precision="bf16-mixed"</code>), and NeMo apply AMP automatically once enabled.</p>
                    </div>
                </section>

                <!-- CPU Offloading -->
                <section id="cpu-offload" class="scroll-mt-6">
                    <h2 class="text-2xl font-bold text-white mb-6 flex items-center gap-3">
                        <span class="w-1.5 h-8 bg-nv-green rounded-full"></span>
                        CPU Offloading
                    </h2>
                    
                    <div class="tech-card p-6 mb-4">
                        <p class="text-slate-300 mb-4">
                            <strong class="text-white">CPU offloading</strong> moves part of the workload—e.g. optimizer states, gradients, or inactive parameters—from GPU memory to CPU RAM. The GPU stays less memory-bound and can fit larger models or larger batches.
                        </p>
                        <ul class="text-slate-400 text-sm space-y-2 list-disc list-inside">
                            <li>Extension of FSDP / DeepSpeed: inactive parameters, gradients, or optimizer states are offloaded to CPU</li>
                            <li>On <strong class="nv-green">Grace Hopper</strong>, NVLink-C2C (~900 GB/s) makes CPU–GPU transfers much faster than PCIe, so offloading is more effective</li>
                            <li><strong class="text-slate-300">SuperOffload</strong> (PyTorch) is built for Grace–Hopper: full fine-tuning of 20B+ models on a single GH200, up to ~4× throughput vs prior ZeRO-Offload, GPU utilization from ~50% to &gt;80%</li>
                            <li>Integrates with DeepSpeed ZeRO Stage 3 and Hugging Face Transformers without model code changes</li>
                        </ul>
                    </div>
                    <div class="warning-card p-5">
                        <p class="text-slate-300 text-sm"><strong class="text-amber-400">Trade-off:</strong> Extra CPU–GPU transfer latency. On systems with slow PCIe, offload can slow training; on Grace Hopper, the fast link makes it a win.</p>
                    </div>
                </section>

                <!-- ZeRO -->
                <section id="zero" class="scroll-mt-6">
                    <h2 class="text-2xl font-bold text-white mb-6 flex items-center gap-3">
                        <span class="w-1.5 h-8 bg-nv-green rounded-full"></span>
                        ZeRO (Zero Redundancy Optimizer)
                    </h2>
                    
                    <div class="tech-card p-6 mb-4">
                        <p class="text-slate-300 mb-4">
                            <strong class="text-white">ZeRO</strong> (DeepSpeed) partitions optimizer states, gradients, and optionally parameters across GPUs so that each device only holds a fraction. This reduces per-GPU memory and enables larger models or larger batch sizes when using multiple GPUs.
                        </p>
                        <ul class="text-slate-400 text-sm space-y-2 list-disc list-inside">
                            <li><strong class="text-slate-300">Stage 1:</strong> Partition optimizer states across ranks</li>
                            <li><strong class="text-slate-300">Stage 2:</strong> Partition gradients as well</li>
                            <li><strong class="text-slate-300">Stage 3:</strong> Partition parameters; each GPU only stores a slice of the model</li>
                            <li>Often combined with <strong class="nv-green">ZeRO-Offload</strong>: offload optimizer states (and optionally more) to CPU to further reduce GPU memory</li>
                            <li>When memory exceeds single-GPU capacity, multi-GPU setups use <strong class="text-slate-300">TP (Tensor Parallelism) / PP (Pipeline Parallelism) / ZeRO</strong> together</li>
                        </ul>
                    </div>
                </section>

                <!-- CUDA Graphs -->
                <section id="cuda-graphs" class="scroll-mt-6">
                    <h2 class="text-2xl font-bold text-white mb-6 flex items-center gap-3">
                        <span class="w-1.5 h-8 bg-nv-green rounded-full"></span>
                        CUDA Graphs
                    </h2>
                    
                    <div class="tech-card p-6 mb-4">
                        <p class="text-slate-300 mb-4">
                            <strong class="text-white">CUDA Graphs</strong> record a sequence of kernel launches and replay them with a single call from the CPU. This cuts <strong class="nv-green">kernel launch overhead</strong> and reduces CPU–GPU synchronization, which helps when the CPU is too slow to keep the GPU busy (the “CPU bottleneck”).
                        </p>
                        <ul class="text-slate-400 text-sm space-y-2 list-disc list-inside">
                            <li>Like a “tape recorder”: record the exact sequence of kernels, then replay it repeatedly</li>
                            <li>Works best with <strong class="text-slate-300">fixed shapes and fixed memory addresses</strong>; dynamic control flow or changing tensor sizes limit usability</li>
                            <li>Does <em>not</em> fuse kernels—it only launches existing kernels more efficiently</li>
                            <li>Use when kernels are already optimized but launch latency is the bottleneck; for general PyTorch, <code class="bg-slate-700 px-1.5 py-0.5 rounded">torch.compile</code> (which can use CUDA Graphs under the hood with <code class="bg-slate-700 px-1.5 py-0.5 rounded">mode="reduce-overhead"</code>) is often the easier choice</li>
                        </ul>
                    </div>
                </section>

                <!-- KV Cache Offload -->
                <section id="kv-cache" class="scroll-mt-6">
                    <h2 class="text-2xl font-bold text-white mb-6 flex items-center gap-3">
                        <span class="w-1.5 h-8 bg-nv-green rounded-full"></span>
                        KV Cache Offload
                    </h2>
                    
                    <div class="tech-card p-6 mb-4">
                        <p class="text-slate-300 mb-4">
                            In autoregressive LLM inference (and some training setups), the <strong class="text-white">KV cache</strong> holds key/value states for all previous tokens. For long sequences or large models, it can exceed GPU memory. <strong class="nv-green">KV cache offloading</strong> moves part of the cache to CPU memory and fetches it as needed.
                        </p>
                        <ul class="text-slate-400 text-sm space-y-2 list-disc list-inside">
                            <li>Uses <strong class="text-slate-300">Unified Memory</strong> or explicit CPU–GPU memory sharing: GPU can access a shared address space; pages are migrated or copied on demand</li>
                            <li>Reduces GPU OOMs and allows running very large models (e.g. <strong class="text-slate-300">Llama 3 70B</strong>) by spilling KV cache (and optionally other tensors) to CPU</li>
                            <li>On <strong class="nv-green">Grace Hopper</strong>, NVLink-C2C makes CPU–GPU transfer fast enough for offloaded KV cache to be practical</li>
                        </ul>
                    </div>
                </section>

                <!-- Selective Profiling -->
                <section id="selective-profiling" class="scroll-mt-6">
                    <h2 class="text-2xl font-bold text-white mb-6 flex items-center gap-3">
                        <span class="w-1.5 h-8 bg-nv-green rounded-full"></span>
                        Selective Profiling
                    </h2>
                    
                    <div class="tech-card p-6 mb-4">
                        <p class="text-slate-300 mb-4">
                            Profiling every iteration of a long LLM training run produces huge traces and can change timing. <strong class="text-white">Selective profiling</strong> limits capture to a chosen range of steps so you get actionable data without oversized files or excessive overhead.
                        </p>
                        <ul class="text-slate-400 text-sm space-y-2 list-disc list-inside">
                            <li>Use environment variables (e.g. <strong class="nv-green">TLLM_PROFILE_START_STOP</strong> in TensorRT-LLM) or profiler APIs to start/stop capture only for specific iterations</li>
                            <li><strong class="text-slate-300">CUDA Profiler API</strong> and <strong class="text-slate-300">Nsight Systems</strong> support toggling profiling on/off in code so you can target a warm-up phase plus a few training steps</li>
                            <li>Keeps profile size manageable and focuses the timeline on the region of interest</li>
                            <li>Combine with <strong class="text-slate-300">NVTX markers</strong> and <strong class="text-slate-300">PyTorch Profiler</strong> for a clear view of which ops and kernels run in the profiled window</li>
                        </ul>
                    </div>
                    <div class="info-card p-5">
                        <p class="text-slate-300 text-sm"><strong class="text-white">Workflow:</strong> Run with profiling disabled for warm-up, enable for a short range (e.g. 2–5 iterations), then inspect the trace in Nsight Systems or Chrome tracing.</p>
                    </div>
                </section>

                <!-- References -->
                <section id="references" class="scroll-mt-6">
                    <h2 class="text-2xl font-bold text-white mb-6 flex items-center gap-3">
                        <span class="w-1.5 h-8 bg-nv-green rounded-full"></span>
                        References
                    </h2>
                    
                    <div class="tech-card p-6">
                        <p class="text-slate-400 text-sm mb-4">This page summarizes techniques described in the following NVIDIA technical blogs:</p>
                        <ul class="space-y-3 text-sm">
                            <li>
                                <a href="https://developer.nvidia.com/blog/profiling-llm-training-workflows-on-nvidia-grace-hopper/" target="_blank" rel="noopener noreferrer" class="ref-link">Profiling LLM Training Workflows on NVIDIA Grace Hopper</a>
                                <span class="text-slate-500 block mt-1">— Nsight Systems, PyTorch Profiler, selective iteration profiling, NVTX, Chrome tracing.</span>
                            </li>
                            <li>
                                <a href="https://developer.nvidia.com/blog/advanced-optimization-strategies-for-llm-training-on-nvidia-grace-hopper" target="_blank" rel="noopener noreferrer" class="ref-link">Advanced Optimization Strategies for LLM Training on NVIDIA Grace Hopper</a>
                                <span class="text-slate-500 block mt-1">— CPU offloading, Unified Memory, AMP, FP8, SuperOffload, KV cache offload.</span>
                            </li>
                        </ul>
                    </div>
                </section>

            </main>
        </div>

        <!-- Footer -->
        <footer class="text-center mt-16 pt-8 border-t border-slate-800 text-sm text-slate-500">
            <p>Part of the <a href="../index.html" class="nv-green hover:underline">AI Performance Design Guide</a></p>
            <p class="mt-2">© 2025. For educational purposes. Content based on NVIDIA Grace Hopper technical blogs.</p>
        </footer>
    </div>

    <script>
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function(e) {
                e.preventDefault();
                const target = document.querySelector(this.getAttribute('href'));
                if (target) {
                    target.scrollIntoView({ behavior: 'smooth', block: 'start' });
                }
            });
        });
    </script>
</body>
</html>
